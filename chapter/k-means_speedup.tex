\section{更快得到\texorpdfstring{$k$}{k}-means的解}
本节分析如何加速$k$-means的求解，目前有两种方法加速，一种是减少数据量，还有一种利用三角不等式。

加速聚类的一个自然的想法是将大数据浓缩，得到小数据，
在小数据上聚类。用小数据上的解作为大数据上的解。由于用于减少数据量的采样方法有很多，这是一个框架性算法，算法列在\ref{alg: reduce_k-means}中。
\begin{algorithm}
    \caption{基于减少数据量的$k$-means算法}\label{alg: reduce_k-means}
    \KwIn{数据集$\mathcal{X}$，类数目$k$，聚类算法$\mathcal{A}_c$，采样算法$\mathcal{A}_s$，采样数$m$}
    \KwOut{$k$个点$C$}
    $S \gets$ $\mathcal{A}_s(\mathcal{X},m)$ \\
    $C \gets$ $\mathcal{A}_c(S,k)$ \\
    \textbf{返回} $C$
\end{algorithm}
在上一节提到的理论保证算法都可以扩展后当$\mathcal{A}_s$，比如将$k$-means++、K-M$\text{C}^2$、AF-KM$\text{C}^2$的seeding数目从$k$增加到$m$，使用$k$-means\(\vert \vert\)采样，将局部搜索的返回$k$个点增加到返回$m$个点，以及经典的均匀采样。虽然理论保证的算法可以扩展为采样算法，但是他们主要目的还是提供理论保证，而有一个技术是专门压缩数据量的，称为Coreset。

通俗来讲，Coreset指的是一些特殊的采样点（可能带权重），给定任意$k$个点，这些采样点到这$k$个点的距离平方和都能够近似所有点到这$k$个点的距离平方和，定义见\ref{def: coreset}。
\begin{definition}[常规Coreset]
    \label{def: coreset}
    带权重集合$S$是数据集$\mathcal{X}$一个$\epsilon$-Coreset，如果对任意集合$C \subset \R^d$，$|C|=k$满足
    \begin{equation}
        |\varphi(\mathcal{X},C) - \psi (S,C)| \leq \epsilon \varphi(\mathcal{X},C)
    \end{equation}
\end{definition}
这是一个非常强的定义，因为它要求对所有$C$都要符合定义的约束，所以符合定义\ref{def: coreset}的Coreset也被称为Strong Coreset。对于Strong Coreset，它的一个良好的性质是采样点上的最优距离平方和就可以近似所有点上的最优距离平方和。
\begin{corollary}[Coreset性质1]
    \label{cor: coreset_property1}
    令$\epsilon \in (0, \frac{1}{3})$，$S$是数据集$\mathcal{X}$的一个$\epsilon$-Coreset，则有
    \begin{equation}
        \varphi(\mathcal{X},C_{\text{OPT}_k}^S) \leq (1+3\epsilon) \varphi(\mathcal{X},C_{\text{OPT}_k}^{\mathcal{X}})
    \end{equation}
\end{corollary}
目前效果很好的$\epsilon$-Coreset基于重要性采样(importance sampling)，算法见文献\cite{bachem2018sampling}中算法9，其采样数目为$O(\frac{k\log k}{\epsilon^2}(dk\log k+\log\frac{1}{\delta}))$，时间复杂度为$O(ndk\log\frac{1}{\delta})$，对于采样点来说近似系数是$1+O(\epsilon)$。该方法的近似系数非常好而花费的时间又是线性的，可以说是目前$k$-means问题中相当好的算法。

虽然常规的Coreset如$\epsilon$-Coreset效果好，但是花费的时间还是稍微有点多，Lightweight Coreset\citing{bachem2017scalable}即是牺牲了一部分聚类的质量来换取速度的提升， 具体来说Lightweight Coreset指的是满足定义\ref{def: lightweight_coreset}的带权点集。
\begin{definition}[Lightweight Coreset]
    \label{def: lightweight_coreset}
    令$\epsilon > 0$，$k \in \N$，$\mathcal{X} \subset \R^d$，它中心点是$\mu(\mathcal{X})$，如果对任意$Q \in \{Q'| Q' \subset \R^d, |Q'| \leq k\}$有
    \begin{equation}
        |\varphi(\mathcal{X},Q) - \psi(S,Q)| \leq \frac{\epsilon}{2}\varphi(\mathcal{X},Q) + \frac{\epsilon}{2}\varphi(\mathcal{X},\mu(\mathcal{X}))
    \end{equation}
    则称$S$是一个$(\epsilon,k)$-Lightweight Coreset
\end{definition}
效仿推论\ref{cor: coreset_property1}，易得其有以下近似系数的推论。
\begin{corollary}[Lightweight Coreset性质]
    如果$S$是一个$(\epsilon,k)$-Lightweight Coreset，有
    \begin{equation}
        \varphi(\mathcal{X},C_{\text{OPT}_k}^S) \leq \varphi(\mathcal{X},C_{\text{OPT}_k}^{\mathcal{X}}) + 4\epsilon\varphi(\mathcal{X},\mu(\mathcal{X}))
    \end{equation}
\end{corollary}
构造该Coreset的算法见算法\ref{alg: Lightweight coreset}，该算法有如下理论保证
\begin{algorithm}
    \caption{Lightweight coreset}\label{alg: Lightweight coreset}
    \KwIn{数据集$\mathcal{X}$, 采样点数目$m$}
    \KwOut{$m$个点 $S$}
    $\mu \gets$ 计算$\mathcal{X}$的均值 \\
    \For{$x \in \mathcal{X}$}{
        $q(x) \gets \frac{1}{2}\text{d}(x,\mu)^2/\sum_{x' \in \mathcal{X}} \text{d}(x',\mu)^2 + \frac{1}{2n}$
    }
    $S \gets $ 依分布$q$从$\mathcal{X}$中采样$m$个点且每个点$x$的权重是$\frac{1}{m.q(x)}$ \\
    \textbf{返回} $S$
\end{algorithm}
\begin{theorem}[Lightweight Coreset理论保证]
    \label{theo: lightweight_coreset}
    令$\epsilon>0$，$\delta>0$，且$k\in \N$。令$\mathcal{X}$是$\R^d$中的数据点，令$S$是算法\ref{alg: Lightweight coreset}的输出。其中采样数
    \begin{equation}
        m \geq c\frac{dk\log k + \log(1/\delta)}{\epsilon^2}
    \end{equation}
    其中$c$是一个常数。这样就可以至少1-$\delta$的概率使得集合$S$是一个$(\epsilon,k)$-Lightweight Coreset。
\end{theorem}
该定理说明只要采样常数个点（与$n$无关），算法就可以在$O(nd)$的时间复杂度里取得好的聚类结果（如果$\varphi(\mathcal{X},C_{\text{OPT}_1}^{\mathcal{X}})/\varphi(\mathcal{X},C_{\text{OPT}_k}^{\mathcal{X}})$不大的话）。特别的，如果前述假设2满足的话，对于采样点来说近似系数就变为了$1+O(k\epsilon)$。%remark: 对于采样点来说近似系数就变为了$1+O(k\epsilon)$ 说法不严谨，\phi(X,C_{opt_k}^S)是$1+O(k\epsilon)$　不是\phi(X,S)

构造 Coreset 的方法除了这里提到的，还有很多， 比如还
可以用$k$-means++构造Coreset\citing{ackermann2012streamkm++}，Coreset作为一个经典的数
据压缩范式还被用于其他机器学习问题中，比如PCA， NNMF（Non Negative Matrix Factorization）等，文献\cite{bachem2017practical}和\cite{feldman2013turning}是很好的参考资料。

除了减少数据量，基于三角不等式也可以加速，由于本论文工作和该方向重合度不高，感兴趣的读者可参考文献\cite{ding2015yinyang,elkan2003using,drake2012accelerated,newling2018novel}。